{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log a trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To set up tracing to LangSmith using the @traceable decorator in Python, first ensure that the LANGSMITH_TRACING environment variable is set to 'true' and the LANGSMITH_API_KEY is configured with your API key. Then, simply decorate any function you want to trace with @traceable. Remember to use the await keyword for asynchronous function calls to ensure traces are logged correctly.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app import langsmith_rag\n",
    "\n",
    "question = \"How do I set up tracing to LangSmith with @traceable?\"\n",
    "langsmith_rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The protagonist of the Attack on Titan anime series is Eren Yeager.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app import langsmith_rag\n",
    "\n",
    "question = \"What is the name of the protagonist in Attack on Titan anime series\"\n",
    "langsmith_rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataset to evaluate this particular step of our application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['a81642dc-48e2-49e9-a191-42b7f6e63c84',\n",
       "  '233361c6-9f71-4948-9a84-cb8cace73dbf',\n",
       "  '3c2900af-206a-4bee-89d1-8d872084f3b4'],\n",
       " 'count': 3}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "example_dataset = [\n",
    "    (\n",
    "        \"How do I set up tracing to LangSmith with @traceable?\",\n",
    "        \"\"\" 2. Log a trace‚Äã\\nOnce you've set up your environment, you can call LangChain runnables as normal.\\nLangSmith will infer the proper tracing config:\\n\\nHow to log and view traces to LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n2. Log a trace‚Äã\\nOnce you've set up your environment, wrap or decorate the custom functions/SDKs you want to trace.\\nLangSmith will then infer the proper tracing config:\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startObservabilityConceptual GuideHow-to GuidesTracingAnnotate code for tracingToggle tracing on and offUpload files with tracesLog traces to specific projectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataTrace JS functions in serverless environmentsMonitoring and automationsTutorialsAdd observability to your LLM applicationEvaluationPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceObservabilityHow-to GuidesTracingAnnotate code for tracingOn this pageAnnotate code for tracing\\nThere are several ways to log traces to LangSmith.\\ntipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions.\\nUse @traceable / traceable‚Äã\\nLangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript.\\nnoteThe LANGSMITH_TRACING environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGSMITH_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.\\nTo log traces to a different project, see this section. \\n\\n\"\"\",\n",
    "        \"To set up tracing to LangSmith using the `@traceable` decorator in Python, first ensure that you have the `LANGSMITH_TRACING` environment variable set to 'true' and the `LANGSMITH_API_KEY` set to your API key. Then, you can simply decorate your functions like this:\\n\\n```python\\nfrom langsmith import traceable\\n\\n@traceable\\ndef my_function():\\n    # Your code here\\n    pass\\n```\\n\\nThis will log traces for `my_function` automatically once the necessary environment variables are configured.\"\n",
    "    ),\n",
    "    (\n",
    "        \"How can I use the tracing context manager?\",\n",
    "        \"\"\"Use the trace context manager (Python only)‚Äã\\nIn Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where:\\n\\nYou want to log traces for a specific block of code.\\nYou want control over the inputs, outputs, and other attributes of the trace.\\nIt is not feasible to use a decorator or wrapper.\\nAny or all of the above.\\n\\nIn some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically.\\nRecently changed behaviorDue to a number of asks for finer-grained control of tracing using the trace context manager,\\nwe changed the behavior of with trace to honor the LANGSMITH_TRACING environment variable in version 0.1.95 of the Python SDK. You can find more details in the release notes.\\nThe recommended way to disable/enable tracing without setting environment variables is to use the with tracing_context context manager, as shown in the example below.\\nPythonTypeScriptThe recommended way to do this in Python is to use the tracing_context context manager. This works for both code annotated with traceable and code within the trace context manager.\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startObservabilityConceptual GuideHow-to GuidesTracingAnnotate code for tracingToggle tracing on and offUpload files with tracesLog traces to specific projectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataTrace JS functions in serverless environmentsMonitoring and automationsTutorialsAdd observability to your LLM applicationEvaluationPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceObservabilityHow-to GuidesTracingTroubleshoot trace nestingOn this pageTroubleshoot trace nesting\\nWhen tracing with the LangSmith SDK, LangGraph, and LangChain, tracing should automatically propagate the correct context so that code executed within a parent trace will be rendered in the expected location in the UI.\\nIf you see a child run go to a separate trace (and appear on the top level), it may be caused by one of the following known \\\"edge cases\\\".\\nPython‚Äã\\nThe following outlines common causes for \\\"split\\\" traces when building with python.\\nContext propagation using asyncio‚Äã\\nWhen using async calls (especially with streaming) in Python versions < 3.11, you may encounter issues with trace nesting. This is because Python's asyncio only added full support for passing context in version 3.11.\\nWhy‚Äã\\nLangChain and LangSmith SDK use contextvars to propagate tracing information implicitly. In Python 3.11 and above, this works seamlessly. However, in earlier versions (3.8, 3.9, 3.10), asyncio tasks lack proper contextvar support, which can lead to disconnected traces.\\nTo resolve‚Äã\\n\\nContext propagation using threading‚Äã\\nIt's common to start tracing and want to apply some parallelism on child tasks all within a single trace. Python's stdlib ThreadPoolExecutor by default breaks tracing.\\nWhy‚Äã\\nPython's contextvars start empty within new threads. Here are two approaches to handle maintain trace contiguity:\\nTo resolve‚Äã\\n\\n\\nUsing LangSmith's ContextThreadPoolExecutor\\nLangSmith provides a ContextThreadPoolExecutor that automatically handles context propagation:\\nfrom langsmith.utils import ContextThreadPoolExecutorfrom langsmith import traceable@traceabledef outer_func():    with ContextThreadPoolExecutor() as executor:        inputs = [1, 2]        r = list(executor.map(inner_func, inputs))@traceabledef inner_func(x):    print(x)outer_func()\\n\\n\\nManually providing the parent run tree\\nAlternatively, you can manually pass the parent run tree to the inner function:\\nfrom langsmith import traceable, get_current_run_treefrom concurrent.futures import ThreadPoolExecutor@traceabledef outer_func():    rt = get_current_run_tree()    with ThreadPoolExecutor() as executor:        r = list(            executor.map(                lambda x: inner_func(x, langsmith_extra={\\\"parent\\\": rt}), [1, 2]            )        )@traceabledef inner_func(x):    print(x)outer_func()\\nIn this approach, we use get_current_run_tree() to obtain the current run tree and pass it to the inner function using the langsmith_extra parameter. \\n\\n \"\"\",\n",
    "        \"You can use the tracing context manager by wrapping the code you want to trace with the `with trace_context:` statement. Here's a simple example:\\n\\n```python\\nfrom langsmith import trace_context\\n\\nwith trace_context:\\n    # Your traceable code here\\n    print(\\\"Tracing this block of code.\\\")\\n``` \\n\\nThis will log traces to LangSmith for the specified code block.\"\n",
    "    ),\n",
    "    (\n",
    "        \"How can I use RunTree?\",\n",
    "        \"\"\"PythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = \\\"Can you summarize this morning's meetings?\\\"# Create a top-level runpipeline = RunTree(  name=\\\"Chat Pipeline\\\",  run_type=\\\"chain\\\",  inputs={\\\"question\\\": question})# This can be retrieved in a retrieval stepcontext = \\\"During this morning's meeting, we solved all world conflict.\\\"messages = [  { \\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { \\\"role\\\": \\\"user\\\", \\\"content\\\": f\\\"Question: {question}\\\\nContext: {context}\\\"}]# Create a child runchild_llm_run = pipeline.create_child(  name=\\\"OpenAI Call\\\",  run_type=\\\"llm\\\",  inputs={\\\"messages\\\": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(  model=\\\"gpt-4o-mini\\\", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.postRun()pipeline.end(outputs={\\\"answer\\\": chat_completion.choices[0].message.content})pipeline.postRun()import OpenAI from \\\"openai\\\";import { RunTree } from \\\"langsmith\\\";// This can be a user input to your appconst question = \\\"Can you summarize this morning's meetings?\\\";const pipeline = new RunTree({  name: \\\"Chat Pipeline\\\",  run_type: \\\"chain\\\",  inputs: { question }});await pipeline.postRun();// This can be retrieved in a retrieval stepconst context = \\\"During this morning's meeting, we solved all world conflict.\\\";const messages = [  { role: \\\"system\\\", content: \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { role: \\\"user\\\", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({  name: \\\"OpenAI Call\\\",\\n\\nPythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = \\\"Can you summarize this morning's meetings?\\\"# Create a top-level runpipeline = RunTree(  name=\\\"Chat Pipeline\\\",  run_type=\\\"chain\\\",  inputs={\\\"question\\\": question})await pipeline.postRun();# This can be retrieved in a retrieval stepcontext = \\\"During this morning's meeting, we solved all world conflict.\\\"messages = [  { \\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { \\\"role\\\": \\\"user\\\", \\\"content\\\": f\\\"Question: {question}\\\\nContext: {context}\\\"}]# Create a child runchild_llm_run = pipeline.create_child(  name=\\\"OpenAI Call\\\",  run_type=\\\"llm\\\",  inputs={\\\"messages\\\": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(  model=\\\"gpt-3.5-turbo\\\", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.postRun()pipeline.end(outputs={\\\"answer\\\": chat_completion.choices[0].message.content})pipeline.postRun()import OpenAI from \\\"openai\\\";import { RunTree } from \\\"langsmith\\\";// This can be a user input to your appconst question = \\\"Can you summarize this morning's meetings?\\\";const pipeline = new RunTree({  name: \\\"Chat Pipeline\\\",  run_type: \\\"chain\\\",  inputs: { question }});// This can be retrieved in a retrieval stepconst context = \\\"During this morning's meeting, we solved all world conflict.\\\";const messages = [  { role: \\\"system\\\", content: \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { role: \\\"user\\\", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({  name: \\\"OpenAI\\n\\nAlternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function.\\n\\nRunTree({  run_type: \\\"llm\\\",  name: \\\"OpenAI Call RunTree\\\",  inputs: { messages },  tags: [\\\"my-tag\\\"],  extra: {metadata: {\\\"my-key\\\": \\\"my-value\\\"}}})await rt.postRun();const chatCompletion = await client.chat.completions.create({  model: \\\"gpt-3.5-turbo\\\",  messages: messages,});// End and submit the runawait rt.end(chatCompletion)await rt.patchRun()from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([(\\\"system\\\", \\\"You are a helpful AI.\\\"),(\\\"user\\\", \\\"{input}\\\")])chat_model = ChatOpenAI()output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\\\"tags\\\": [\\\"top-level-tag\\\"], \\\"metadata\\\": {\\\"top-level-key\\\": \\\"top-level-value\\\"}})# Tags and metadata can also be passed at runtimechain.invoke({\\\"input\\\": \\\"What is the meaning of life?\\\"}, {\\\"tags\\\": [\\\"shared-tags\\\"], \\\"metadata\\\": {\\\"shared-key\\\": \\\"shared-value\\\"}})import { ChatOpenAI } from \\\"@langchain/openai\\\";import { ChatPromptTemplate } from \\\"@langchain/core/prompts\\\";import { StringOutputParser } from \\\"@langchain/core/output_parsers\\\";const prompt = ChatPromptTemplate.fromMessages([[\\\"system\\\", \\\"You are a helpful AI.\\\"],[\\\"user\\\", \\\"{input}\\\"]])const model = new ChatOpenAI({ modelName: \\\"gpt-3.5-turbo\\\" });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\\\"tags\\\": [\\\"top-level-tag\\\"], \\\"metadata\\\": {\\\"top-level-key\\\": \\\"top-level-value\\\"}});// Tags and metadata can also be \\n\\n\"\"\",\n",
    "        \"You can use `RunTree` by creating a pipeline where you define its name, type, and inputs. For example:\\n\\n```python\\nfrom langsmith.run_trees import RunTree\\n\\npipeline = RunTree(name=\\\"My Pipeline\\\", run_type=\\\"chain\\\", inputs={\\\"question\\\": \\\"Your question here\\\"})\\n```\\n\\nYou can then create child runs and manage them accordingly within the pipeline.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"technical Questions\"\n",
    "\n",
    "# Create dataset\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name, description=\"Technical questions about LangSmith\"\n",
    ")\n",
    "\n",
    "# Prepare inputs and outputs\n",
    "inputs = [{\"question\": q, \"context\": c} for q, c, _ in example_dataset]\n",
    "outputs = [{\"output\": o} for _, _, o in example_dataset]\n",
    "\n",
    "# Create examples in the dataset\n",
    "client.create_examples(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    dataset_id=dataset.id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['d3997fa6-5527-4e9d-9409-6c9b11d4e595',\n",
       "  '5f46e675-eb9b-4cc5-9696-12e47f34cf18',\n",
       "  '8e0bb5d1-731b-43f9-ac42-bbdd1a7a95e3',\n",
       "  '229b1bb0-3352-44cf-97fe-7864f73cc0a4',\n",
       "  'a178b340-ef1e-4de9-8002-66df5d4ffdb4'],\n",
       " 'count': 5}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "example_dataset = attack_on_titan_dataset = [\n",
    "    (\n",
    "        \"What are the three main walls that protect humanity in Attack on Titan and what are their key characteristics?\",\n",
    "        \"\"\"The three concentric walls are Maria, Rose, and Sina, named after the royal family.‚Äã\\nWall Maria is the outermost and largest wall, approximately 50 meters tall and spanning a territory with a radius of 480 kilometers. It was the first line of defense and contained the most territory, including the Shiganshina District.‚Äã\\nWall Rose is the middle wall, also 50 meters tall, with a territory radius of 380 kilometers. It served as the secondary line of defense after the fall of Wall Maria and contained more urbanized areas like Trost District.‚Äã\\nWall Sina is the innermost and smallest wall, protecting the innermost territory with a radius of 250 kilometers. It contained the capital and royal government in Mitras, housing the nobility and wealthy citizens.‚Äã\\nThe walls were constructed approximately 100 years before the main story begins using the Founding Titan's power to harden countless Colossus Titans into the wall structures. Each wall district contained distinct communities, with Wall Maria being primarily agricultural and Wall Sina being the political and economic center. The walls created a false sense of security that was shattered when the Colossal Titan first appeared in year 845.‚Äã\\nSkip to main contentGo to Episode GuideSearchRegionGlobalGo to SiteQuick startStory OverviewConceptual GuideCharacter ProfilesWall Structure HistoryTitan BiologyMilitary OrganizationPolitical FactionsKey LocationsHistorical EventsMajor BattlesTechnology DevelopmentCultural AspectsReferenceMaterialsTimeline AnalysisTheology and BeliefsMilitary StrategyTitan ResearchOn this pageThe Three Walls of Paradise‚Äã\\nAttack on Titan's central premise revolves around humanity's last remnants surviving within three massive concentric walls. These walls represent both physical protection and psychological prisons, creating the series' distinctive setting and driving much of the plot's conflict and themes of freedom versus security.\"\"\",\n",
    "        \"The three main walls are Wall Maria (outermost, 50m tall, 480km radius, agricultural areas), Wall Rose (middle, 50m tall, 380km radius, urban districts), and Wall Sina (innermost, 50m tall, 250km radius, capital city). They were built using the Founding Titan's power to harden Colossus Titans approximately 100 years before the story begins, with Wall Maria falling first in year 845 when the Colossal Titan appeared.\"\n",
    "    ),\n",
    "    (\n",
    "        \"How does the Titan transformation process work and what are the physical requirements for shifting?\",\n",
    "        \"\"\"Titan shifters transform through self-inflicted injury that draws blood, typically by biting their hand or another body part.‚Äã\\nThe transformation requires a clear mental image or purpose of what the shifter wants to create, determining the Titan's size, appearance, and capabilities. The process generates massive amounts of steam and heat, with the Titan body materializing around the shifter who typically resides in the nape region.‚Äã\\nPhysical requirements include being a subject of Ymir (descendant of the original Titan progenitor), having consumed a previous shifter or being injected with Titan spinal fluid, and sufficient willpower to control the transformation. Shifters can maintain their form for varying durations depending on their skill and energy reserves, with excessive use leading to exhaustion and reduced lifespan.‚Äã\\nThe transformation creates an explosive release of energy that can be dangerous to nearby people and structures. Advanced shifters can perform partial transformations or control specific aspects like hardening. Each of the Nine Titans has unique characteristics that manifest differently during transformation, from the Attack Titan's muscular build to the Cart Titan's quadrupedal form.‚Äã\\nSkip to main contentGo to Episode GuideSearchRegionGlobalGo to SiteQuick startStory OverviewConceptual GuideCharacter ProfilesWall Structure HistoryTitan BiologyMilitary OrganizationPolitical FactionsKey LocationsHistorical EventsMajor BattlesTechnology DevelopmentCultural AspectsReferenceMaterialsTimeline AnalysisTheology and BeliefsMilitary StrategyTitan ResearchOn this pageTitan Shifting Mechanics‚Äã\\nThe transformation process represents both a weapon and a curse, granting immense power at the cost of the user's lifespan and humanity. Mastering this ability is central to the series' combat and strategic elements, with characters like Eren Yeager undergoing extensive training to control their Titan forms effectively.\"\"\",\n",
    "        \"Titan transformation requires a subject of Ymir to inflict self-injury (usually biting their hand) while having a clear mental image of their Titan form. The shifter must have inherited Titan powers through consumption or injection, and the transformation generates explosive energy and steam. Shifters reside in the nape of their Titan and can maintain the form for limited durations, with excessive use reducing their 13-year lifespan curse.\"\n",
    "    ),\n",
    "    (\n",
    "        \"What are the Nine Titans and their distinct abilities?\",\n",
    "        \"\"\"The Nine Titans are: Founding Titan (controls other Titans and memories), Attack Titan (sees future memories and fights for freedom), Colossus Titan (massive size and steam explosion ability), Armored Titan (protective plating), Female Titan (ability to attract Titans and partial crystallization), Beast Titan (varies by user, typically primate-like with throwing accuracy), Jaw Titan (hardened jaw and claws for speed), Cart Titan (endurance and quadrupedal movement), and War Hammer Titan (creates structures from hardened Titan flesh).‚Äã\\nEach Titan possesses unique characteristics that reflect their purpose and history. The Founding Titan, held by the royal family, can control Pure Titans and alter memories of Subjects of Ymir. The Attack Titan inherently seeks freedom and can access memories of future inheritors. The Colossus Titan stands at 60 meters tall and can release immense steam. The Armored Titan's body is covered in protective plates except at joints.‚Äã\\nThe Female Titan can selectively harden body parts and attract Pure Titans with her scream. The Beast Titan's form varies by bloodline but typically demonstrates exceptional throwing ability. The Jaw Titan possesses incredible biting power and speed. The Cart Titan exhibits exceptional endurance, able to remain transformed for months. The War Hammer Titan can create weapons and structures from distance-controlled hardening.‚Äã\\nSkip to main contentGo to Episode GuideSearchRegionGlobalGo to SiteQuick startStory OverviewConceptual GuideCharacter ProfilesWall Structure HistoryTitan BiologyMilitary OrganizationPolitical FactionsKey LocationsHistorical EventsMajor BattlesTechnology DevelopmentCultural AspectsReferenceMaterialsTimeline AnalysisTheology and BeliefsMilitary StrategyTitan ResearchOn this pageThe Nine Titan Shifters‚Äã\\nThese Titans have been passed down through generations since Ymir Fritz first gained Titan powers 2,000 years ago. Their abilities have been used in various conflicts, particularly the war between Marley and other nations, before becoming central to the Paradis Island conflict.\"\"\",\n",
    "        \"The Nine Titans are: Founding Titan (memory control), Attack Titan (future memories), Colossus Titan (60m height, steam), Armored Titan (protective plating), Female Titan (crystallization, Titan attraction), Beast Titan (primate form, throwing), Jaw Titan (speed, biting power), Cart Titan (endurance, quadrupedal), and War Hammer Titan (hardened weapon creation). They originated from Ymir Fritz 2,000 years ago and are passed down through inheritance.\"\n",
    "    ),\n",
    "    (\n",
    "        \"What are the different military branches in Paradis and their respective functions?\",\n",
    "        \"\"\"The Paradis military consists of three branches: Survey Corps, Garrison Regiment, and Military Police Brigade.‚Äã\\nThe Survey Corps specializes in external expeditions beyond the walls to reclaim territory, research Titans, and seek freedom. They develop advanced combat techniques like Vertical Maneuvering Equipment and suffer the highest casualty rates. Led by commanders like Erwin Smith and later Hange Zo√´, they become central to uncovering the truth about Titans and the outside world.‚Äã\\nThe Garrison Regiment is responsible for wall defense and maintenance, operating the wall cannons and guarding the gates. They are the largest branch and handle civilian protection during Titan attacks. Their duties include wall repairs, gate operations, and internal security. Key members include Dot Pixis and Hannes.‚Äã\\nThe Military Police Brigade maintains internal order and protects the royal government, serving as an elite force that typically avoids Titan combat. They are stationed primarily within Wall Sina and often come from wealthy families. The Special Operations Squad within the Military Police handles special assignments, including protecting important figures.‚Äã\\nSkip to main contentGo to Episode GuideSearchRegionGlobalGo to SiteQuick startStory OverviewConceptual GuideCharacter ProfilesWall Structure HistoryTitan BiologyMilitary OrganizationPolitical FactionsKey LocationsHistorical EventsMajor BattlesTechnology DevelopmentCultural AspectsReferenceMaterialsTimeline AnalysisTheology and BeliefsMilitary StrategyTitan ResearchOn this pageParadis Island Military Structure‚Äã\\nRecruits choose their branch after graduation from training corps based on their rankings, with top performers typically selecting the Military Police. This structure reflects Paradis society's priorities before the truth about the outside world is revealed, with each branch representing different philosophies about humanity's survival.\"\"\",\n",
    "        \"The three military branches are: Survey Corps (external expeditions, Titan research, highest casualties), Garrison Regiment (wall defense, largest branch, civilian protection), and Military Police Brigade (internal security, elite force, protects royalty). Graduates choose branches based on training corps rankings, with top performers typically joining the Military Police to avoid Titan combat.\"\n",
    "    ),\n",
    "    (\n",
    "        \"What is the Paths dimension and how does it connect all Subjects of Ymir?\",\n",
    "        \"\"\"Paths is a extradimensional plane that exists outside normal time and space, connecting all Subjects of Ymir across 2,000 years of history.‚Äã\\nThis dimension appears as an endless desert under a starry sky with a giant tree-like structure at its center, from which countless paths extend to every Subject of Ymir. The Founding Titan can access and manipulate this dimension to alter biology, memories, and even control Titans. All Titan transformations and hardening abilities draw material through Paths.‚Äã\\nThe Paths dimension transcends linear time, allowing memories to be sent backward and forward through generations. Ymir Fritz remains in this dimension, physically creating Titans from sand for 2,000 years until she is freed. The coordinate point where the Founding Titan can access Paths is typically located where a royal-blooded Titan and the Founding Titan make physical contact.‚Äã\\nCommunication through Paths appears as vivid visions or dreams, with significant events like the Rumbling being announced simultaneously to all Subjects of Ymir worldwide. The dimension serves as the source of Titan powers and the mechanism through which the Curse of Ymir limits shifter lifespans to 13 years after inheritance.‚Äã\\nSkip to main contentGo to Episode GuideSearchRegionGlobalGo to SiteQuick startStory OverviewConceptual GuideCharacter ProfilesWall Structure HistoryTitan BiologyMilitary OrganizationPolitical FactionsKey LocationsHistorical EventsMajor BattlesTechnology DevelopmentCultural AspectsReferenceMaterialsTimeline AnalysisTheology and BeliefsMilitary StrategyTitan ResearchOn this pageThe Paths Dimension‚Äã\\nPaths represents the metaphysical foundation of Titan biology and the interconnected nature of the Eldian people. Its discovery fundamentally changes understanding of the series' events and provides the mechanism for large-scale phenomena like memory manipulation and the worldwide Rumbling activation.\"\"\",\n",
    "        \"Paths is an extradimensional plane connecting all Subjects of Ymir across time, appearing as an endless desert with a central tree. It allows the Founding Titan to control Titans, alter memories and biology, and is where Ymir Fritz creates Titans for 2,000 years. The dimension transcends linear time and serves as the source of all Titan powers and the 13-year lifespan curse.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Attack On Titan Questions\"\n",
    "\n",
    "# Create dataset\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name, description=\"General Information about Attack On titan\"\n",
    ")\n",
    "\n",
    "# Prepare inputs and outputs\n",
    "inputs = [{\"question\": q, \"context\": c} for q, c, _ in example_dataset]\n",
    "outputs = [{\"output\": o} for _, _, o in example_dataset]\n",
    "\n",
    "# Create examples in the dataset\n",
    "client.create_examples(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    dataset_id=dataset.id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update our Application to use Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to pretty much define the same RAG application as before - with one crucial improvement.\n",
    "\n",
    "Instead of pulling our `RAG_PROMPT` from utils.py, we're going to connect to the Prompt Hub in LangSmith.\n",
    "\n",
    "Let's add the code snippet that will pull down our prompt that we just iterated on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
    "from langsmith import Client\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"attackontitanprompt\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langsmith import traceable\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "APP_VERSION = 1.0\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
    "from langsmith import Client\n",
    "client = Client()\n",
    "# Pull just the prompt template without model binding\n",
    "prompt = client.pull_prompt(\"attackontitanprompt\")  # No include_model\n",
    "\n",
    "def get_vector_db_retriever():\n",
    "    persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n",
    "    embd = OpenAIEmbeddings()\n",
    "\n",
    "    # If vector store exists, then load it\n",
    "    if os.path.exists(persist_path):\n",
    "        vectorstore = SKLearnVectorStore(\n",
    "            embedding=embd,\n",
    "            persist_path=persist_path,\n",
    "            serializer=\"parquet\"\n",
    "        )\n",
    "        return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "    # Otherwise, index LangSmith documents and create new vector store\n",
    "    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\", continue_on_failure=True)\n",
    "    ls_docs = ls_docs_sitemap_loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=0\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(ls_docs)\n",
    "\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=embd,\n",
    "        persist_path=persist_path,\n",
    "        serializer=\"parquet\"\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_openai` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    # Use our prompt pulled from Prompt Hub\n",
    "    formatted_prompt = prompt.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    messages = convert_prompt_to_openai_format(formatted_prompt)[\"messages\"]\n",
    "    return call_openai(messages)\n",
    "\n",
    "\"\"\"\n",
    "call_openai\n",
    "- Returns the chat completion output from OpenAI\n",
    "\"\"\"\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\n",
    "        \"ls_provider\": MODEL_PROVIDER,\n",
    "        \"ls_model_name\": MODEL_NAME\n",
    "    }\n",
    ")\n",
    "def call_openai(messages: List[dict]) -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The protagonist of \"Attack on Titan\" is Eren Yeager. He is determined to fight against the Titans and uncover the truth behind their existence. His journey drives much of the story\\'s plot.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who is the protagonist of Attack on Ttian\"\n",
    "langsmith_rag(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
