{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can connect our application to LangSmith's Prompt Hub, which will allow us to test and iterate on our prompts within LangSmith, and pull our improvements directly into our application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull a prompt from Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in a prompt from Prompt Hub by pasting in the code snippet from the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"poet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we pulled - note that we did not get the model, so this is just a StructuredPrompt and not runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredPrompt(input_variables=['question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'poet', 'lc_hub_commit_hash': '15675cccba351ece10bdc51d09fcbc03c37c273d70be9b5a3b2497d4a130875c'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a professional poet and specialize in writing 2 line clever and sarcastic poems to any prompt given.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'answer', 'description': 'Extracts the answer.', 'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from the LLM to the user'}}, 'required': [], 'strict': True, 'additionalProperties': False}, structured_output_kwargs={})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now let's hydrate our prompt by calling .invoke() with our inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a professional poet and specialize in writing 2 line clever and sarcastic poems to any prompt given.', additional_kwargs={}, response_metadata={}), HumanMessage(content='How is the weather?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydrated_prompt = prompt.invoke({\"question\": \"How is the weather?\"})\n",
    "hydrated_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's pass those messages to OpenAI and see what we get back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-CQBnSjJ2aX7m3rFGMBhMPD4SGmE7Q', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The forecast says rain, but I’m dressed for the sun,  \\nSo I’ll just admire the tempest while I binge and have fun.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1760357910, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_51db84afab', usage=CompletionUsage(completion_tokens=28, prompt_tokens=38, total_tokens=66, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\n",
    "converted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=converted_messages,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Extra: LangChain Only] Pulling down the Model Configuration\n",
    "\n",
    "We can also pull down the saved model configuration as a LangChain RunnableBinding when we use `include_model=True`. This allows us to run our prompt template directly with the saved model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.13/json/decoder.py:345: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructuredPrompt(input_variables=['question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'poet', 'lc_hub_commit_hash': '15675cccba351ece10bdc51d09fcbc03c37c273d70be9b5a3b2497d4a130875c'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a professional poet and specialize in writing 2 line clever and sarcastic poems to any prompt given.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'answer', 'description': 'Extracts the answer.', 'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from the LLM to the user'}}, 'required': [], 'strict': True, 'additionalProperties': False}, structured_output_kwargs={})\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x1294a1a70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x1295fc050>, root_client=<openai.OpenAI object at 0x1294a1810>, root_async_client=<openai.AsyncOpenAI object at 0x1294b7d90>, model_name='gpt-5-mini', temperature=1.0, model_kwargs={'extra_headers': {}}, openai_api_key=SecretStr('**********'), top_p=1.0), kwargs={'response_format': {'type': 'json_schema', 'json_schema': {'name': 'answer', 'description': 'Extracts the answer.', 'strict': True, 'schema': {'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from the LLM to the user'}}, 'required': [], 'strict': True, 'additionalProperties': False}}}, 'ls_structured_output_format': {'kwargs': {'method': 'json_schema', 'strict': None}, 'schema': {'type': 'function', 'function': {'name': 'answer', 'description': 'Extracts the answer.', 'parameters': {'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from the LLM to the user'}}, 'required': [], 'strict': True, 'additionalProperties': False}}}}}, config={}, config_factories=[])\n",
       "| JsonOutputParser()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "hub.pull(\"poet\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredPrompt(input_variables=['question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'poet', 'lc_hub_commit_hash': '15675cccba351ece10bdc51d09fcbc03c37c273d70be9b5a3b2497d4a130875c'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a professional poet and specialize in writing 2 line clever and sarcastic poems to any prompt given.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'answer', 'description': 'Extracts the answer.', 'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from the LLM to the user'}}, 'required': [], 'strict': True, 'additionalProperties': False}, structured_output_kwargs={})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out your prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a professional poet and specialize in writing 2 line clever and sarcastic poems to any prompt given.', additional_kwargs={}, response_metadata={}), HumanMessage(content='How is the weather', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"question\": \"How is the weather\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull down a specific commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull down a specific commit from the Prompt Hub by pasting in the code snippet from the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.13/json/decoder.py:345: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    }
   ],
   "source": [
    "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
    "from langsmith import Client\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"poet:b47ddda7\", include_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this commit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime polymorphism in Java, often referred to as dynamic method dispatch, is a fundamental concept in object-oriented programming. It allows a method to perform different tasks based on the object that it is acting upon at runtime. This is typically achieved via method overriding and interfaces.\n",
      "\n",
      "### How Runtime Polymorphism Works:\n",
      "\n",
      "1. **Method Overriding**: This occurs when a subclass provides a specific implementation of a method that is already defined in its parent class. The method invoked is determined by the actual object type at runtime rather than the reference type.\n",
      "\n",
      "2. **Reference Type**: A reference variable can point to an object of its own class or any subclass. When a method is called on that reference, Java checks which implementation to invoke based on the actual object type.\n",
      "\n",
      "### Example of Runtime Polymorphism:\n",
      "\n",
      "Here’s a simple example to illustrate runtime polymorphism:\n",
      "\n",
      "```java\n",
      "class Animal {\n",
      "    void sound() {\n",
      "        System.out.println(\"Animal makes a sound\");\n",
      "    }\n",
      "}\n",
      "\n",
      "class Dog extends Animal {\n",
      "    void sound() {\n",
      "        System.out.println(\"Dog barks\");\n",
      "    }\n",
      "}\n",
      "\n",
      "class Cat extends Animal {\n",
      "    void sound() {\n",
      "        System.out.println(\"Cat meows\");\n",
      "    }\n",
      "}\n",
      "\n",
      "public class PolymorphismExample {\n",
      "    public static void main(String[] args) {\n",
      "        Animal myAnimal;\n",
      "\n",
      "        myAnimal = new Dog();\n",
      "        myAnimal.sound(); // Output: Dog barks\n",
      "\n",
      "        myAnimal = new Cat();\n",
      "        myAnimal.sound(); // Output: Cat meows\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "### Explanation of the Example:\n",
      "\n",
      "1. **Superclass (Animal)**: Contains a method `sound()`, which can be overridden.\n",
      "2. **Subclasses (Dog and Cat)**: Each subclass provides its own implementation of the `sound()` method.\n",
      "3. **Reference Variable (`myAnimal`)**: This variable is of type `Animal`, but it can hold references to any subclass type (like `Dog` or `Cat`).\n",
      "4. **Method Invocation**: At runtime, when `myAnimal.sound()` is called, Java determines which version of the method to execute based on the actual object (`Dog` or `Cat`) that `myAnimal` is pointing to at that moment.\n",
      "\n",
      "### Benefits of Runtime Polymorphism:\n",
      "\n",
      "- **Flexibility**: The code can be more flexible and reusable. You can write a method that works with the superclass type, but it can operate on any subclass types.\n",
      "- **Maintainability**: Modifications can be made to subclasses without affecting the code that uses the superclass, leading to easier maintenance and upgrades.\n",
      "\n",
      "In summary, runtime polymorphism plays a crucial role in enabling flexible and dynamic program behavior based on object types during execution, promoting a more abstract and modular approach in implementing functionalities.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Get just the prompt without the model binding to avoid structured output issues\n",
    "from langchain import hub\n",
    "prompt_only = hub.pull(\"poet\")  # This gets just the prompt template\n",
    "\n",
    "hydrated_prompt = prompt_only.invoke({\"question\": \"What is run time polymorphism\"})\n",
    "converted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=converted_messages,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also easily update your prompts in the hub programmatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/hindi-rag-prompt/d83fe14b?organizationId=6176107e-37d8-4611-acb8-9f68bf99512f'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "\n",
    "client=Client()\n",
    "\n",
    "hindi_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your users can only speak Hindi, make sure you only answer your users with Hindi.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "hindi_prompt_template = ChatPromptTemplate.from_template(hindi_prompt)\n",
    "client.push_prompt(\"hindi-rag-prompt\", object=hindi_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/hindi-runnable-sequence/5611dea6?organizationId=6176107e-37d8-4611-acb8-9f68bf99512f'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "client=Client()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "hindi_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your users can only speak Hindi, make sure you only answer your users with Hindi.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "hindi_prompt_template = ChatPromptTemplate.from_template(hindi_prompt)\n",
    "chain = hindi_prompt_template | model\n",
    "client.push_prompt(\"hindi-runnable-sequence\", object=chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
